name: oumi-bulk-inference

num_nodes: 1
resources:
  cloud: gcp
  accelerators: "A100-80GB:8"
  use_spot: true
  disk_size: 2000 # Disk size in GBs

# Upload a working directory to remote ~/sky_workdir.
working_dir: .

# Mount local files.
file_mounts:
  ~/.cache/huggingface/token: ~/.cache/huggingface/token

storage_mounts:
  /output_gcs_bucket:
    source: gs://jgreer-test-sky-bucket-out # GCS bucket to store output
    store: gcs
  /model_bucket:
    source: gs://oumi-dev-us-central1 # GCS bucket to store output
    store: gcs
  /input_gcs_bucket:
    source: gs://jgreer-test-sky-bucket # GCS bucket that contains our input data
    store: gcs

envs:
  TOKENIZERS_PARALLELISM: false
  INFERENCE_CONFIG: configs/recipes/llama3_1/inference/8b_infer.yaml # Inference config file
  HF_DATASETS_TRUST_REMOTE_CODE: 1

setup: |
  set -e
  pip install uv && uv pip install '.[gpu]'

run: |
  set -e  # Exit if any command failed.
  source ./configs/examples/misc/sky_init.sh
  python ./scripts/inference/gcp_inference.py $SKYPILOT_NUM_GPUS_PER_NODE
